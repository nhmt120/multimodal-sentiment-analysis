{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom  matplotlib import pyplot as plt\n\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:05.105424Z","iopub.execute_input":"2022-06-15T11:21:05.106008Z","iopub.status.idle":"2022-06-15T11:21:11.340465Z","shell.execute_reply.started":"2022-06-15T11:21:05.105899Z","shell.execute_reply":"2022-06-15T11:21:11.339180Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"mvsa_single_data_path = '../input/mvsasingle/MVSA_Single/data'\nmvsa_single_label_path = '../input/mvsasingle/MVSA_Single/labelResultAll.txt'\nmvsa_multiple_data_path = '../input/mvsamultiple/MVSA/data'\nmvsa_multiple_label_path = '../input/mvsamultiple/MVSA/labelResultAll.txt'\n\nIMAGE_SIZE = (224, 224)\nNUM_CHANNELS = 3","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.342309Z","iopub.execute_input":"2022-06-15T11:21:11.343428Z","iopub.status.idle":"2022-06-15T11:21:11.349493Z","shell.execute_reply.started":"2022-06-15T11:21:11.343386Z","shell.execute_reply":"2022-06-15T11:21:11.348218Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def read_text_file(path, multi_line=False):\n    if multi_line == True:\n        lines = open(path, 'r', encoding='latin-1').readlines()\n        lines = [line.rstrip('\\n') for line in lines]\n        return lines\n    return open(path, 'r', encoding='latin-1').read()\n\ndef read_image_file(path):\n    try:\n        image = cv2.imread(path, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n        invalid_ID = -1\n    except:\n        image = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], NUM_CHANNELS))\n        invalid_ID = os.path.split(path)[1].split('.')[0]\n    return image, invalid_ID\n\ndef read_labels_file(path):\n    dataframe = pd.read_csv(path, sep=\"\\s+|,\", engine=\"python\")\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.350942Z","iopub.execute_input":"2022-06-15T11:21:11.352063Z","iopub.status.idle":"2022-06-15T11:21:11.363393Z","shell.execute_reply.started":"2022-06-15T11:21:11.352021Z","shell.execute_reply":"2022-06-15T11:21:11.362254Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# since there are 3 annotators defining each data labels in the MVSA-Multiple dataset\n# we take the labels pair that 2 out 3 annotators agree on as the valid labels pair, remove otherwise\ndef merge_multi_label(dataframe):\n    anno_1 = list(dataframe.iloc[:, 1:3].itertuples(index=False, name=None))\n    anno_2 = list(dataframe.iloc[:, 3:5].itertuples(index=False, name=None))\n    anno_3 = list(dataframe.iloc[:, 5:7].itertuples(index=False, name=None))\n    IDs = list(dataframe.iloc[:, 0])\n    \n    valid_pairs = []\n    \n    for i in range(len(anno_1)):\n        pairs = [anno_1[i], anno_2[i], anno_3[i]]\n        ID = IDs[i]\n        \n        valid_pair = tuple([pair for pair in pairs if pairs.count(pair) > 1])\n        \n        if len(valid_pair) == 0:\n            valid_pair = (ID, 'invalid', 'invalid')\n        else:\n            valid_pair = (ID, valid_pair[0][0], valid_pair[0][1])\n        valid_pairs.append(valid_pair)\n        \n    valid_dataframe = pd.DataFrame(valid_pairs, columns=['ID', 'text', 'image'])\n    return valid_dataframe\n\ndef multimodal_label(text_label, image_label):\n    if text_label == image_label:\n        label = text_label\n    elif (text_label == 'positive' and image_label == 'negative') or (text_label == 'negative' and image_label == 'positive'):\n        label = 'invalid'\n    elif (text_label == 'neutral' and image_label != 'neutral') or (text_label != 'neutral' or image_label == 'neutral'):\n        label = image_label if text_label == 'neutral' else text_label\n    return label","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.365447Z","iopub.execute_input":"2022-06-15T11:21:11.365874Z","iopub.status.idle":"2022-06-15T11:21:11.381634Z","shell.execute_reply.started":"2022-06-15T11:21:11.365837Z","shell.execute_reply":"2022-06-15T11:21:11.379956Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_data_paths(path, extension):\n    paths = os.listdir(path)\n    paths.sort(key = lambda x : int(x.split('.')[0]))\n    paths = list(filter(lambda x: x.endswith(extension), paths))\n    paths = [os.path.join(path, x) for x in paths]\n    return paths\n\ndef get_image_with_id(path):\n    filename = os.path.split(path)[1]\n    ID = int(filename.split('.')[0])\n    image = read_image_file(path)\n    return (ID, image)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.383241Z","iopub.execute_input":"2022-06-15T11:21:11.383695Z","iopub.status.idle":"2022-06-15T11:21:11.399206Z","shell.execute_reply.started":"2022-06-15T11:21:11.383657Z","shell.execute_reply":"2022-06-15T11:21:11.398194Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_labels(path, multiple=False, mappings=False):\n    dataframe = read_labels_file(path)\n    \n    if multiple == True:\n        dataframe = merge_multi_label(dataframe)\n\n    labels = []\n    for label_pair in dataframe.iloc[:, 1:].values:\n        label = multimodal_label(label_pair[0], label_pair[1])\n        labels.append(label)\n        \n    if mappings == True:\n        label_map = {}\n        for i in range(len(labels)):\n            ID = dataframe.iloc[i, 0]\n            label_map[ID] = labels[i]            \n        return label_map\n    \n    return np.array(labels, dtype='object')\n\ndef create_text_data(path):\n    texts = []\n    text_paths = get_data_paths(path, '.txt')\n    \n    print('Read text data')\n    for text_path in tqdm(text_paths):\n        text = read_text_file(text_path).rstrip('\\n')\n        texts.append(text)\n        \n    return np.array(texts)\n\ndef create_image_data(path):\n#     images = []\n    images = np.array([])\n\n    invalid_indices = []\n    image_paths = get_data_paths(path, '.jpg')\n\n    print('Read image data')\n    for image_path in tqdm(image_paths):\n        image, invalid_ID = read_image_file(image_path)\n        if images.shape[0] == 0:\n            images = np.array([image])\n        else:\n            images = np.concatenate((images, [image]))\n#         images.append(image)\n\n        if invalid_ID != -1:\n            invalid_indices.append(invalid_ID)\n            \n    return images, invalid_indices","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.400515Z","iopub.execute_input":"2022-06-15T11:21:11.401142Z","iopub.status.idle":"2022-06-15T11:21:11.415336Z","shell.execute_reply.started":"2022-06-15T11:21:11.401093Z","shell.execute_reply":"2022-06-15T11:21:11.414306Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"a = np.array([])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.416609Z","iopub.execute_input":"2022-06-15T11:21:11.417552Z","iopub.status.idle":"2022-06-15T11:21:11.433811Z","shell.execute_reply.started":"2022-06-15T11:21:11.417500Z","shell.execute_reply":"2022-06-15T11:21:11.432868Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# a = np.array([np.zeros((5, 5, 3))])\n# b = np.zeros((5, 5, 3))\n# c = np.concatenate((a, [b]))\n# d = np.concatenate((c, [b]))\n# d.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.435195Z","iopub.execute_input":"2022-06-15T11:21:11.435675Z","iopub.status.idle":"2022-06-15T11:21:11.446433Z","shell.execute_reply.started":"2022-06-15T11:21:11.435643Z","shell.execute_reply":"2022-06-15T11:21:11.445582Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# if extracted_data.shape[0] == 0:\n#     extracted_data = np.concatenate(([extracted_data], [features]), 1)\n# else:\n#     extracted_data = np.concatenate((extracted_data, [features]), 0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.447884Z","iopub.execute_input":"2022-06-15T11:21:11.448256Z","iopub.status.idle":"2022-06-15T11:21:11.460681Z","shell.execute_reply.started":"2022-06-15T11:21:11.448221Z","shell.execute_reply":"2022-06-15T11:21:11.459647Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def invalid_indices(labels):\n    invalid_indices = [i for i in range(labels.shape[0]) if labels[i] == 'invalid']\n    return indices\n\ndef remove_invalid(data, indices):\n#     indices = invalid_indices(labels)\n    new_data =  np.delete(data, indices)\n    return new_data","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.463272Z","iopub.execute_input":"2022-06-15T11:21:11.463819Z","iopub.status.idle":"2022-06-15T11:21:11.475203Z","shell.execute_reply.started":"2022-06-15T11:21:11.463755Z","shell.execute_reply":"2022-06-15T11:21:11.474215Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def save_text_file(filename, lines):\n    with open(filename, 'w', encoding='latin1') as f:\n        f.write('\\n'.join(lines))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:11.476853Z","iopub.execute_input":"2022-06-15T11:21:11.477478Z","iopub.status.idle":"2022-06-15T11:21:11.493993Z","shell.execute_reply.started":"2022-06-15T11:21:11.477431Z","shell.execute_reply":"2022-06-15T11:21:11.492845Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# mvsa_single_texts = create_text_data(mvsa_single_data_path)\nmvsa_single_images, mvsa_single_images_invalid_indices = create_image_data(mvsa_single_data_path)\n# mvsa_single_labels = create_labels(mvsa_single_label_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T11:21:15.798633Z","iopub.execute_input":"2022-06-15T11:21:15.799106Z","iopub.status.idle":"2022-06-15T11:39:55.692975Z","shell.execute_reply.started":"2022-06-15T11:21:15.799070Z","shell.execute_reply":"2022-06-15T11:39:55.691025Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Read image data\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4869/4869 [18:39<00:00,  4.35it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# mvsa_multiple_texts = create_text_data(mvsa_multiple_data_path)\nmvsa_multiple_images, mvsa_multiple_images_invalid_indices = create_image_data(mvsa_multiple_data_path)\n# mvsa_multiple_labels = create_labels(mvsa_multiple_label_path, multiple=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:47:28.672938Z","iopub.execute_input":"2022-06-15T10:47:28.673710Z","iopub.status.idle":"2022-06-15T10:51:41.092964Z","shell.execute_reply.started":"2022-06-15T10:47:28.673669Z","shell.execute_reply":"2022-06-15T10:51:41.091880Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Read image data\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 12238/19600 [02:32<01:44, 70.18it/s]Premature end of JPEG file\n100%|██████████| 19600/19600 [04:11<00:00, 77.87it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Remove invalid data","metadata":{}},{"cell_type":"code","source":"mvsa_single_labels_invalid_indices = [i for i in range(mvsa_single_labels.shape[0]) if mvsa_single_labels[i] == 'invalid']\n\nmvsa_single_invalid_indices = []\nmvsa_single_invalid_indices.extend(mvsa_single_labels_invalid_indices)\nmvsa_single_invalid_indices.extend(mvsa_single_images_invalid_indices)\nmvsa_single_invalid_indices = list(set(mvsa_single_invalid_indices))\n\nmvsa_single_texts_valid = remove_invalid(mvsa_single_texts, mvsa_single_invalid_indices)\nmvsa_single_images_valid = remove_invalid(mvsa_single_images, mvsa_single_invalid_indices)\nmvsa_single_labels_valid = remove_invalid(mvsa_single_labels, nmvsa_single_invalid_indicesvalid_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mvsa_multiple_labels_invalid_indices = [i for i in range(mvsa_multiple_labels.shape[0]) if mvsa_multiple_labels[i] == 'invalid']\n\nmvsa_multiple_invalid_indices = []\nmvsa_multiple_invalid_indices.extend(mvsa_multiple_labels_invalid_indices)\nmvsa_multiple_invalid_indices.extend(mvsa_multiple_images_invalid_indices)\nmvsa_multiple_invalid_indices = list(set(mvsa_multiple_invalid_indices))\n\nmvsa_multiple_texts_valid = remove_invalid(mvsa_multiple_texts, mvsa_multiple_invalid_indices)\nmvsa_multiple_images_valid = remove_invalid(mvsa_multiple_images, mvsa_multiple_invalid_indices)\nmvsa_multiple_labels_valid = remove_invalid(mvsa_multiple_labels, mvsa_multiple_invalid_indices)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_text_file('./mvsa-single-texts.txt', mvsa_single_texts_valid)\nnp.save('./mvsa-single-images.npy', mvsa_single_images_valid)\nsave_text_file('./mvsa-single-labels.txt', mvsa_single_labels_valid)\n\nsave_text_file('./mvsa-multiple-texts.txt', mvsa_multiple_texts_valid)\nnp.save('./mvsa-multiple-images.npy', mvsa_multiple_images_valid)\nsave_text_file('./mvsa-multiple-labels.txt', mvsa_multiple_labels_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mvsa_single_texts_loaded = read_text_file('./mvsa-single-texts.txt', multi_line=True)\nmvsa_single_images_loaded = np.load('./mvsa-single-images.npy')\nmvsa_single_labels_loaded = read_text_file('./mvsa-single-labels.txt', multi_line=True)\n\nprint((mvsa_single_texts_valid == mvsa_single_texts_loaded).all())\nprint((mvsa_single_images_loaded == mvsa_single_images_valid).all())\nprint((mvsa_single_labels_valid == mvsa_single_labels_loaded).all())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mvsa_multiple_texts_loaded = read_text_file('./mvsa-multiple-texts.txt', multi_line=True)\nmvsa_multiple_images_loaded = np.load('./mvsa-multiple-images.npy')\nmvsa_multiple_labels_loaded = read_text_file('./mvsa-multiple-labels.txt', multi_line=True)\n\nprint((mvsa_multiple_texts_valid == mvsa_multiple_texts_loaded).all())\nprint((mvsa_multiple_images_loaded == mvsa_multiple_images_valid).all())\nprint((mvsa_multiple_labels_valid == mvsa_multiple_labels_loaded).all())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############\n# df_labels = read_labels_file('../input/mvsasingle/MVSA_Single/labelResultAll.txt')\n# df_labels['text'].count\n# label_text_count = df_labels.value_counts('text')\n# label_image_count = df_labels.value_counts('image')\n# label_text_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_ids = []\n# image_ids = []\n# for i in range(len(texts)):\n#     if texts[i][0] != images[i][0] != read_labels_file(label_path)['ID'][i]:\n#         print('here')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# u, c, i = np.unique(texts, return_index=True, return_counts=True, axis=1)\n# dup = u[c > 1]\n# u[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def display_sample(text, image):\n#     plt.imshow(image)\n#     print('Text:', text)\n#     plt.show()\n# display_sample(texts_with_id[2][1], images_with_id[2][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shapes = []\n# for image in images_with_id:\n#     shapes.append(image.shape)\n# shapes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# raw_texts[raw_texts.duplicated('Text')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len_split = lambda x: len(x.split())\n# print()\n# print('Min number of words in text:', raw_texts['Text'].apply(len_split).min())\n# print('Max number of words in text:', raw_texts['Text'].apply(len_split).max())\n# print('Average number of words in text:', round(raw_texts['Text'].apply(len_split).mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drafts","metadata":{}},{"cell_type":"code","source":"# # mvsa_single_images.tofile('./mvsa-single-images.npy')\n# np.save('./mvsa-single-images.npy', mvsa_single_images)\n# mvsa_single_images_loaded = np.load('./mvsa-single-images.npy')\n# (mvsa_single_images_loaded == mvsa_single_images).all()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = read_labels_file(mvsa_single_label_path).iloc[:, 1:].values\n# for i in a:\n#     if i[0] == 'negative' and i[1] == 'positive':\n#         print('here')\n#     elif i[0] == 'positive' and i[1] == 'negative':\n#         print('hereee')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label_mappings = create_labels(mvsa_single_label_path, mappings=True)\n\n# image_dataset = make_dataset([os.path.join(mvsa_single_label_path, str(ID) + '.jpg') for ID in label_mappings.keys()],\n#                              list(label_mappings.values()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = get_image_with_id(os.path.join(mvsa_single_data_path, '1.jpg'))\n# tf.keras.utils.array_to_img(a[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label_mappings = create_labels(mvsa_single_label_path, mappings=True)\n\n# image_dataset = make_dataset([os.path.join(mvsa_single_label_path, str(ID) + '.jpg') for ID in label_mappings.keys()],\n#                              list(label_mappings.values()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = os.listdir(mvsa_single_data_path)\n# a.sort(key = lambda x: int(x.split('.')[0]))\n# list(filter(lambda x: x.endswith('.jpg'), a))\n# a = [os.path.join(mvsa_single_data_path, i) for i in a]\n# a","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # a = create_labels(mvsa_multiple_label_path, multiple=True, mappings=True)\n# def process_image(path, label):\n# #     filename = os.path.split(path)[1]\n# #     ID = int(filename.split('.')[0])\n# #     return read_image_file(path), label_map[ID]\n#     return read_image_file(path), label\n\n# def make_dataset(images, labels):\n#     dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n# #     dataset = dataset.shuffle(len(images))\n#     dataset = dataset.map(process_image)#, num_parallel_calls=AUTOTUNE)\n# #     dataset = dataset.batch(100).prefetch(tf.data.AUTOTUNE)\n#     return dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMAGE_SIZE = (224, 224)\n# def read_image_file(path):\n#     image = cv2.imread(path, cv2.COLOR_BGR2RGB)\n# #     image = np.array(image)#.astype('float32')\n# #     image = cv2.resize(image, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n# #     image /= 255\n# #     print(image.shape)\n# #     ax = plt.subplot(1,2,1)\n# #     plt.imshow(image)\n# #     image = cv2.resize(image, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n# #     ax = plt.subplot(1,2,2)\n# #     plt.imshow(image)\n# #     print(image.shape)\n#     return image\n# read_image_file('../input/mvsasingle/MVSA_Single/data/10.jpg')      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def read_text_file(path, multi_line=False):\n# #     with open(path, 'r', encoding='latin-1') as f:\n# #         if multi_line == True:\n# #             lines = f.readlines()\n# #             lines = [line.rstrip('\\n') for line in lines]\n# #             return lines\n# #         return f.read()\n    \n#     if multi_line == True:\n#         lines = open(path, 'r', encoding='latin-1').readlines()\n#         lines = [line.rstrip('\\n') for line in lines]\n#         return lines\n#     return open(path, 'r', encoding='latin-1').read()\n\n# def read_image_file(path):\n#     try:\n#         image = cv2.imread(path, cv2.COLOR_BGR2RGB)\n#         image = cv2.resize(image, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n#     except:\n#         image = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], NUM_CHANNELS))\n#         ID = os.path.split(path)[1].split('.')[0]\n#         invalid_indices.append(ID)\n# #     image = tf.io.read_file(path)\n# #     image = tf.image.decode_jpeg(image, channels=NUM_CHANNELS)\n# #     image = tf.image.resize(image, IMAGE_SIZE)\n#     return image\n\n# def read_labels_file(path):\n#     dataframe = pd.read_csv(path, sep=\"\\s+|,\", engine=\"python\")\n#     return dataframe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_labels(path, multiple=False, mappings=False):\n#     dataframe = read_labels_file(path)\n    \n#     if multiple == True:\n#         dataframe = merge_multi_label(dataframe)\n        \n#     labels = []\n#     for _, row in dataframe.iterrows():\n#         label = multimodal_label(row['text'], row['image'])\n#         labels.append(label)\n        \n#     if mappings == True:\n#         label_map = {}\n#         for i in range(len(labels)):\n#             ID = dataframe.iloc[i, 0]\n#             label_map[ID] = labels[i]            \n#         return label_map\n#     return np.array(labels, dtype='object')\n\n# def create_text_data(path):\n#     texts = []\n    \n#     print('Read text data')\n#     # read data along with its filename as ID\n#     for filename in tqdm(os.listdir(path)):\n#         ID = int(filename.split('.')[0])\n#         file_path = os.path.join(path, filename)\n#         if filename.endswith('txt'):\n#             text = read_text_file(file_path)\n#             texts.append((ID, text))\n\n#     # Sort data by its ID\n#     get_ID = lambda x : x[0]\n#     texts.sort(key=get_ID)\n    \n#     # return data without ID\n#     texts = np.array([text[1].rstrip('\\n') for text in texts])\n\n#     return texts\n\n# def create_image_data(path):\n#     images = []\n     \n#     image_paths = os.listdir(path)\n#     image_paths.sort(key = lambda x : int(x.split('.')[0]))\n#     image_paths = list(filter(lambda x: x.endswith('.jpg'), image_paths))\n#     image_paths = [os.path.join(path, x) for x in image_paths]\n# #     print(image_path)\n    \n#     print('Read image data')\n#     # read data along with its filename as ID\n#     for image_path in tqdm(image_paths):\n# #         ID = int(filename.split('.')[0])\n# #         file_path = os.path.join(path, filename)\n# #         gc.collect()\n# #         if filename.endswith('jpg'):\n# #         image_with_id = get_image_with_id(image_path)\n# #         images.append(image_with_id)\n#         image = read_image_file(image_path)\n#         images.append(image)\n# #         del image\n# #         gc.collect()\n        \n    \n# #     # Sort data by its ID\n# #     get_ID = lambda x : x[0]\n# #     images.sort(key=get_ID)\n    \n#     # return data without ID\n# #     images = np.array([image[1] for image in images], dtype='object')\n\n#     return images\n\n# def get_image_with_id(path):\n#     filename = os.path.split(path)[1]\n#     ID = int(filename.split('.')[0])\n#     image = read_image_file(path)\n#     return (ID, image)","metadata":{},"execution_count":null,"outputs":[]}]}