{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234af72f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:47.412614Z",
     "iopub.status.busy": "2022-07-01T14:13:47.412058Z",
     "iopub.status.idle": "2022-07-01T14:13:56.750943Z",
     "shell.execute_reply": "2022-07-01T14:13:56.749923Z"
    },
    "papermill": {
     "duration": 9.35184,
     "end_time": "2022-07-01T14:13:56.753542",
     "exception": false,
     "start_time": "2022-07-01T14:13:47.401702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 61\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import os\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(SEED) \n",
    "    python_random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    \n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import tensorflow_addons as tfa\n",
    "import h5py\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers,regularizers,constraints\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Reshape, Input, Embedding, Flatten, Dense, Dropout, BatchNormalization, Activation #, merge\n",
    "from keras.layers import TimeDistributed, LSTM, GRU, Bidirectional, Convolution1D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import RepeatVector #, Reshape\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from tensorflow.python.keras.layers import Layer, InputSpec, Lambda\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "\n",
    "# from tensorflow.keras import Model\n",
    "# from attention import Attention_input1, Attention_input2\n",
    "# from keras.optimizers import SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ccfabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.771399Z",
     "iopub.status.busy": "2022-07-01T14:13:56.770131Z",
     "iopub.status.idle": "2022-07-01T14:13:56.777872Z",
     "shell.execute_reply": "2022-07-01T14:13:56.776843Z"
    },
    "papermill": {
     "duration": 0.018495,
     "end_time": "2022-07-01T14:13:56.779867",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.761372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_hdf5(path):\n",
    "    read_file = h5py.File(path, 'r')\n",
    "    \n",
    "    feature_names = list(read_file.keys())\n",
    "    loaded_data = []\n",
    "    \n",
    "    for name in feature_names:\n",
    "        dataset = read_file[name][:]\n",
    "        if dataset.dtype == np.dtype('object'):\n",
    "            dataset = np.array([x.decode('UTF-8') for x in dataset])            \n",
    "        loaded_data.append((name, dataset))\n",
    "\n",
    "    return loaded_data\n",
    "\n",
    "def loadz(path):\n",
    "    data = np.load(path)['arr_0']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82154b38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.796232Z",
     "iopub.status.busy": "2022-07-01T14:13:56.795890Z",
     "iopub.status.idle": "2022-07-01T14:13:56.805839Z",
     "shell.execute_reply": "2022-07-01T14:13:56.804829Z"
    },
    "papermill": {
     "duration": 0.020595,
     "end_time": "2022-07-01T14:13:56.808121",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.787526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labels(path):\n",
    "    data = read_hdf5(path)\n",
    "\n",
    "    for x in data:\n",
    "        if x[0] == 'multimodal-labels':\n",
    "            labels = x[1]\n",
    "        if x[0] == 'text-labels':\n",
    "            text_labels = x[1]\n",
    "        if x[0] == 'image-labels':\n",
    "            image_labels = x[1]\n",
    "        \n",
    "    return labels, text_labels, image_labels\n",
    "\n",
    "def merge_mvsa(mvsa_single, mvsa_multiple):\n",
    "    mvsa = np.concatenate((mvsa_single, mvsa_multiple), axis=0)\n",
    "    return mvsa\n",
    "\n",
    "def load_mvsa_feature(feature_name, merge=False):\n",
    "    folder_path = os.path.join('../input/mvsa-features/', feature_name)\n",
    "    single_file = 'mvsa-single-{}.npz'.format(feature_name)\n",
    "    multiple_file = 'mvsa-multiple-{}.npz'.format(feature_name)\n",
    "    mvsa_single = loadz(os.path.join(folder_path, single_file))\n",
    "    mvsa_multiple = loadz(os.path.join(folder_path, multiple_file))\n",
    "    \n",
    "    if merge == True:\n",
    "        return merge_mvsa(mvsa_single, mvsa_multiple)\n",
    "    \n",
    "    return mvsa_single, mvsa_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67525446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.824277Z",
     "iopub.status.busy": "2022-07-01T14:13:56.823929Z",
     "iopub.status.idle": "2022-07-01T14:13:56.829671Z",
     "shell.execute_reply": "2022-07-01T14:13:56.828642Z"
    },
    "papermill": {
     "duration": 0.016316,
     "end_time": "2022-07-01T14:13:56.831769",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.815453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# e.g. validation_split=0.1 -----> 8:1:1 ratio of train, val, test\n",
    "def split_data(data, validation_split):\n",
    "    num_val = int(validation_split * data.shape[0])\n",
    "    data_train = data[:-(num_val*2)]\n",
    "    data_val = data[-(num_val*2):-(num_val)]\n",
    "    data_test = data[-num_val:]\n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da47061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.848295Z",
     "iopub.status.busy": "2022-07-01T14:13:56.847930Z",
     "iopub.status.idle": "2022-07-01T14:13:56.855540Z",
     "shell.execute_reply": "2022-07-01T14:13:56.854289Z"
    },
    "papermill": {
     "duration": 0.018608,
     "end_time": "2022-07-01T14:13:56.857866",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.839258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(weights, probs):\n",
    "    '''\n",
    "    weights: weights list (or array)\n",
    "    probs: probability distributions array\n",
    "    '''\n",
    "    output_probs = []\n",
    "    weighted_probs = [probs[i] * weights[i] for i in range(len(weights))]\n",
    "    for i in range(len(probs[0])):\n",
    "        sum_prob = np.zeros(len(probs[0][0]))\n",
    "        for j in range(len(weights)):\n",
    "            sum_prob = np.sum((sum_prob, weighted_probs[j][i]), axis=0)\n",
    "        output_probs.append(sum_prob)\n",
    "    return np.asarray(output_probs, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74e23d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.874508Z",
     "iopub.status.busy": "2022-07-01T14:13:56.873492Z",
     "iopub.status.idle": "2022-07-01T14:13:56.880322Z",
     "shell.execute_reply": "2022-07-01T14:13:56.879504Z"
    },
    "papermill": {
     "duration": 0.01713,
     "end_time": "2022-07-01T14:13:56.882329",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.865199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_average_weights(*scores, inverse=False):\n",
    "    # inverse weights in case of the smaller score value, the bigger weight\n",
    "    weights = []\n",
    "    for score in scores:\n",
    "        weights.append(score/np.sum(scores))\n",
    "    \n",
    "    if inverse == True:\n",
    "        inverse_weights = []\n",
    "        inverse = [1/weight for weight in weights]\n",
    "        for inv in inverse:\n",
    "            inverse_weights.append(inv/np.sum(inverse))\n",
    "        weights = inverse_weights\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a56d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.899282Z",
     "iopub.status.busy": "2022-07-01T14:13:56.898370Z",
     "iopub.status.idle": "2022-07-01T14:13:56.905666Z",
     "shell.execute_reply": "2022-07-01T14:13:56.904706Z"
    },
    "papermill": {
     "duration": 0.018117,
     "end_time": "2022-07-01T14:13:56.907924",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.889807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_text(input_shape):\n",
    "    f1_score = tfa.metrics.F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "    \n",
    "    text_input = Input(shape=input_shape)\n",
    "    reshape_text = Reshape((1, -1)) (text_input)\n",
    "    lstm = LSTM(NUM_LSTM) (reshape_text)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax') (lstm)\n",
    "    \n",
    "    model = Model(text_input, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score]) # f1 #tf.keras.metrics.AUC()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9176591f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.924268Z",
     "iopub.status.busy": "2022-07-01T14:13:56.923919Z",
     "iopub.status.idle": "2022-07-01T14:13:56.929832Z",
     "shell.execute_reply": "2022-07-01T14:13:56.928880Z"
    },
    "papermill": {
     "duration": 0.016206,
     "end_time": "2022-07-01T14:13:56.931677",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.915471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_text_no_lstm(input_shape):\n",
    "    f1_score = tfa.metrics.F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "    \n",
    "    text_input = Input(shape=input_shape)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax') (text_input)\n",
    "    \n",
    "    model = Model(text_input, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score]) # f1 #tf.keras.metrics.AUC()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a8b7fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.949273Z",
     "iopub.status.busy": "2022-07-01T14:13:56.948415Z",
     "iopub.status.idle": "2022-07-01T14:13:56.955016Z",
     "shell.execute_reply": "2022-07-01T14:13:56.954122Z"
    },
    "papermill": {
     "duration": 0.017734,
     "end_time": "2022-07-01T14:13:56.957015",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.939281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_image(input_shape):\n",
    "    f1_score = tfa.metrics.F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "    \n",
    "    image_input = Input(shape=input_shape)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax') (image_input)\n",
    "    \n",
    "    model = Model(image_input, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score]) # f1 #tf.keras.metrics.AUC()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef9642c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.973107Z",
     "iopub.status.busy": "2022-07-01T14:13:56.972774Z",
     "iopub.status.idle": "2022-07-01T14:13:56.980377Z",
     "shell.execute_reply": "2022-07-01T14:13:56.979429Z"
    },
    "papermill": {
     "duration": 0.018061,
     "end_time": "2022-07-01T14:13:56.982400",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.964339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_IF(text_shape, image_shape):\n",
    "    f1_score = tfa.metrics.F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "    \n",
    "    image_input = Input(shape=image_shape)\n",
    "    text_input = Input(shape=text_shape)\n",
    "    text_reshape = Reshape((1, -1)) (text_input)\n",
    "    text_lstm = LSTM(NUM_LSTM) (text_reshape)\n",
    "#     text_lstm = Dropout(DROPOUT_RATE) (text_lstm)\n",
    "    text_image_concat = tf.keras.layers.Concatenate(axis=1)([text_lstm, image_input])\n",
    "    concat_self_attention = tf.keras.layers.Attention() ([text_image_concat, text_image_concat])\n",
    "#     concat_self_attention = Dropout(DROPOUT_RATE) (concat_self_attention)\n",
    "    concat_softmax = Dense(NUM_CLASSES, activation='softmax') (concat_self_attention)\n",
    "    \n",
    "    model = Model([text_input, image_input], concat_softmax)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score]) # f1 #tf.keras.metrics.AUC()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fedada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:56.999846Z",
     "iopub.status.busy": "2022-07-01T14:13:56.998856Z",
     "iopub.status.idle": "2022-07-01T14:13:57.006785Z",
     "shell.execute_reply": "2022-07-01T14:13:57.005914Z"
    },
    "papermill": {
     "duration": 0.018392,
     "end_time": "2022-07-01T14:13:57.008788",
     "exception": false,
     "start_time": "2022-07-01T14:13:56.990396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_IF_no_lstm(text_shape, image_shape):\n",
    "    f1_score = tfa.metrics.F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "    \n",
    "    image_input = Input(shape=image_shape)\n",
    "    text_input = Input(shape=text_shape)\n",
    "#     text_reshape = Reshape((1, -1)) (text_input)\n",
    "#     text_lstm = LSTM(NUM_LSTM) (text_reshape)\n",
    "#     text_lstm = Dropout(DROPOUT_RATE) (text_lstm)\n",
    "    text_image_concat = tf.keras.layers.Concatenate(axis=1)([text_input, image_input])\n",
    "    concat_self_attention = tf.keras.layers.Attention() ([text_image_concat, text_image_concat])\n",
    "#     concat_self_attention = Dropout(DROPOUT_RATE) (concat_self_attention)\n",
    "    concat_softmax = Dense(NUM_CLASSES, activation='softmax') (concat_self_attention)\n",
    "    \n",
    "    model = Model([text_input, image_input], concat_softmax)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score]) # f1 #tf.keras.metrics.AUC()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b1d8b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.026032Z",
     "iopub.status.busy": "2022-07-01T14:13:57.025208Z",
     "iopub.status.idle": "2022-07-01T14:13:57.044091Z",
     "shell.execute_reply": "2022-07-01T14:13:57.043121Z"
    },
    "papermill": {
     "duration": 0.03015,
     "end_time": "2022-07-01T14:13:57.046410",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.016260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def old_run_and_evaluate_HF(name, X1, X2, y, verbose=0):\n",
    "    # train and test only with multimodal labels\n",
    "    ''' \n",
    "    X1: text input\n",
    "    X2: image input\n",
    "    y: labels\n",
    "    verbose: 0 or 1 to print tracking on progress\n",
    "    '''\n",
    "    y = le.fit_transform(y)\n",
    "    y = to_categorical(np.asarray(y))\n",
    "    \n",
    "    X1_train, X1_val, X1_test = split_data(X1, VALIDATION_SPLIT)\n",
    "    X2_train, X2_val, X2_test = split_data(X2, VALIDATION_SPLIT)\n",
    "    y_train, y_val, y_test = split_data(y, VALIDATION_SPLIT)\n",
    "    \n",
    "    model_text = create_model_text(X1_train.shape[1:])\n",
    "    model_image = create_model_image(X2_train.shape[1:])\n",
    "    model_IF = create_model_IF(X1_train.shape[1:], X2_train.shape[1:])\n",
    "\n",
    "    early_stopping1 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "    early_stopping2 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "    early_stopping3 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "\n",
    "    checkpoint_text = ModelCheckpoint('./model_checkpoint/{}-text.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "    checkpoint_image = ModelCheckpoint('./model_checkpoint/{}-image.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "    checkpoint_IF = ModelCheckpoint('./model_checkpoint/{}-IF.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "\n",
    "    history_text = model_text.fit(X1_train, y_train, validation_data=(X1_val, y_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_text, early_stopping1])\n",
    "    \n",
    "    history_image = model_image.fit(X2_train, y_train, validation_data=(X2_val, y_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_image, early_stopping2])\n",
    "    \n",
    "    history_IF = model_IF.fit([X1_train, X2_train], y_train, validation_data=([X1_val, X2_val], y_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_IF, early_stopping3])\n",
    "    \n",
    "    best_val_loss_text = np.min(history_text.history['val_loss'])\n",
    "    best_val_loss_image = np.min(history_image.history['val_loss'])\n",
    "    best_val_loss_IF = np.min(history_IF.history['val_loss'])\n",
    "    \n",
    "    weights = get_average_weights(best_val_loss_text, best_val_loss_image, best_val_loss_IF,\n",
    "                                  inverse=True)\n",
    "\n",
    "    model_text = load_model('./model_checkpoint/{}-text.h5'.format(name))\n",
    "    model_image = load_model('./model_checkpoint/{}-image.h5'.format(name))\n",
    "    model_IF = load_model('./model_checkpoint/{}-IF.h5'.format(name))\n",
    "\n",
    "    y_pred_text = model_text.predict(X1_test)\n",
    "    y_pred_image = model_image.predict(X2_test)\n",
    "    y_pred_IF = model_IF.predict([X1_test, X2_test])\n",
    "    \n",
    "    y_pred = weighted_average(weights, np.asarray([y_pred_text, y_pred_image, y_pred_IF], dtype='float32'))\n",
    "    \n",
    "    best_epoch_text = np.argmin(history_text.history['val_loss'])\n",
    "    best_epoch_image = np.argmin(history_image.history['val_loss'])\n",
    "    best_epoch_IF = np.argmin(history_IF.history['val_loss'])\n",
    "\n",
    "    print('Checkpoint of text model loaded at epoch:', best_epoch_text)\n",
    "    print('Checkpoint of image model loaded at epoch:', best_epoch_image)\n",
    "    print('Checkpoint of IF model loaded at epoch:', best_epoch_IF)\n",
    "\n",
    "    return evaluate_LF(y_test, y_pred, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b48728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.063258Z",
     "iopub.status.busy": "2022-07-01T14:13:57.062613Z",
     "iopub.status.idle": "2022-07-01T14:13:57.080931Z",
     "shell.execute_reply": "2022-07-01T14:13:57.079970Z"
    },
    "papermill": {
     "duration": 0.029356,
     "end_time": "2022-07-01T14:13:57.083272",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.053916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_and_evaluate_HF(name, X1, X2, y, y1, y2, verbose=0):\n",
    "    # train with separate labels of each modality\n",
    "    # test with multimodal labels\n",
    "    ''' \n",
    "    X1: text input\n",
    "    X2: image input\n",
    "    y: multimodal labels\n",
    "    y1: text labels\n",
    "    y2: image labels\n",
    "    verbose: 0 or 1 to print tracking on progress\n",
    "    '''\n",
    "    y = le.fit_transform(y)\n",
    "    y = to_categorical(np.asarray(y))\n",
    "    \n",
    "    y1 = le.fit_transform(y1)\n",
    "    y1 = to_categorical(np.asarray(y1))\n",
    "    \n",
    "    y2 = le.fit_transform(y2)\n",
    "    y2 = to_categorical(np.asarray(y2))\n",
    "    \n",
    "    X1_train, X1_val, X1_test = split_data(X1, VALIDATION_SPLIT)\n",
    "    X2_train, X2_val, X2_test = split_data(X2, VALIDATION_SPLIT)\n",
    "    y_train, y_val, y_test = split_data(y, VALIDATION_SPLIT)\n",
    "    y1_train, y1_val, y1_test = split_data(y1, VALIDATION_SPLIT)\n",
    "    y2_train, y2_val, y2_test = split_data(y2, VALIDATION_SPLIT)\n",
    "\n",
    "    model_text = create_model_text(X1_train.shape[1:])\n",
    "    model_image = create_model_image(X2_train.shape[1:])\n",
    "    model_IF = create_model_IF(X1_train.shape[1:], X2_train.shape[1:])\n",
    "\n",
    "    early_stopping1 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "    early_stopping2 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "    early_stopping3 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "\n",
    "    checkpoint_text = ModelCheckpoint('./model_checkpoint/{}-text.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "    checkpoint_image = ModelCheckpoint('./model_checkpoint/{}-image.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "    checkpoint_IF = ModelCheckpoint('./model_checkpoint/{}-IF.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "\n",
    "    history_text = model_text.fit(X1_train, y1_train, validation_data=(X1_val, y1_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_text, early_stopping1])\n",
    "    \n",
    "    history_image = model_image.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_image, early_stopping2])\n",
    "    \n",
    "    history_IF = model_IF.fit([X1_train, X2_train], y_train, validation_data=([X1_val, X2_val], y_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_IF, early_stopping3])\n",
    "    \n",
    "    best_val_loss_text = np.min(history_text.history['val_loss'])\n",
    "    best_val_loss_image = np.min(history_image.history['val_loss'])\n",
    "    best_val_loss_IF = np.min(history_IF.history['val_loss'])\n",
    "    \n",
    "    weights = get_average_weights(best_val_loss_text, best_val_loss_image, best_val_loss_IF,\n",
    "                                  inverse=True)\n",
    "\n",
    "    model_text = load_model('./model_checkpoint/{}-text.h5'.format(name))\n",
    "    model_image = load_model('./model_checkpoint/{}-image.h5'.format(name))\n",
    "    model_IF = load_model('./model_checkpoint/{}-IF.h5'.format(name))\n",
    "\n",
    "    y_pred_text = model_text.predict(X1_test)\n",
    "    y_pred_image = model_image.predict(X2_test)\n",
    "    y_pred_IF = model_IF.predict([X1_test, X2_test])\n",
    "    \n",
    "    y_pred = weighted_average(weights, np.asarray([y_pred_text, y_pred_image, y_pred_IF], dtype='float32'))\n",
    "    \n",
    "    best_epoch_text = np.argmin(history_text.history['val_loss'])\n",
    "    best_epoch_image = np.argmin(history_image.history['val_loss'])\n",
    "    best_epoch_IF = np.argmin(history_IF.history['val_loss'])\n",
    "\n",
    "    print('Checkpoint of text model loaded at epoch:', best_epoch_text)\n",
    "    print('Checkpoint of image model loaded at epoch:', best_epoch_image)\n",
    "    print('Checkpoint of IF model loaded at epoch:', best_epoch_IF)\n",
    "\n",
    "    return evaluate_LF(y_test, y_pred, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b778278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.100122Z",
     "iopub.status.busy": "2022-07-01T14:13:57.099522Z",
     "iopub.status.idle": "2022-07-01T14:13:57.119471Z",
     "shell.execute_reply": "2022-07-01T14:13:57.118513Z"
    },
    "papermill": {
     "duration": 0.03144,
     "end_time": "2022-07-01T14:13:57.122280",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.090840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_and_evaluate_HF_no_lstm(name, X1, X2, y, y1, y2, verbose=0):\n",
    "    ''' \n",
    "    X1: text input\n",
    "    X2: image input\n",
    "    y: multimodal labels\n",
    "    y1: text labels\n",
    "    y2: image labels\n",
    "    verbose: 0 or 1 to print tracking on progress\n",
    "    '''\n",
    "    y = le.fit_transform(y)\n",
    "    y = to_categorical(np.asarray(y))\n",
    "    \n",
    "    y1 = le.fit_transform(y1)\n",
    "    y1 = to_categorical(np.asarray(y1))\n",
    "    \n",
    "    y2 = le.fit_transform(y2)\n",
    "    y2 = to_categorical(np.asarray(y2))\n",
    "    \n",
    "    X1_train, X1_val, X1_test = split_data(X1, VALIDATION_SPLIT)\n",
    "    X2_train, X2_val, X2_test = split_data(X2, VALIDATION_SPLIT)\n",
    "    y_train, y_val, y_test = split_data(y, VALIDATION_SPLIT)\n",
    "    y1_train, y1_val, y1_test = split_data(y1, VALIDATION_SPLIT)\n",
    "    y2_train, y2_val, y2_test = split_data(y2, VALIDATION_SPLIT)\n",
    "\n",
    "    model_text = create_model_text_no_lstm(X1_train.shape[1:])\n",
    "    model_image = create_model_image(X2_train.shape[1:])\n",
    "    model_IF = create_model_IF_no_lstm(X1_train.shape[1:], X2_train.shape[1:])\n",
    "\n",
    "    early_stopping1 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "    early_stopping2 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "    early_stopping3 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=EARLY_STOPPING)\n",
    "\n",
    "    checkpoint_text = ModelCheckpoint('./model_checkpoint/{}-text.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "    checkpoint_image = ModelCheckpoint('./model_checkpoint/{}-image.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "    checkpoint_IF = ModelCheckpoint('./model_checkpoint/{}-IF.h5'.format(name), save_best_only=True, verbose=verbose)\n",
    "\n",
    "    history_text = model_text.fit(X1_train, y1_train, validation_data=(X1_val, y1_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_text, early_stopping1])\n",
    "    \n",
    "    history_image = model_image.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_image, early_stopping2])\n",
    "    \n",
    "    history_IF = model_IF.fit([X1_train, X2_train], y_train, validation_data=([X1_val, X2_val], y_val), \n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=verbose,\n",
    "                        callbacks=[checkpoint_IF, early_stopping3])\n",
    "    \n",
    "    best_val_loss_text = np.min(history_text.history['val_loss'])\n",
    "    best_val_loss_image = np.min(history_image.history['val_loss'])\n",
    "    best_val_loss_IF = np.min(history_IF.history['val_loss'])\n",
    "    \n",
    "    weights = get_average_weights(best_val_loss_text, best_val_loss_image, best_val_loss_IF,\n",
    "                                  inverse=True)\n",
    "\n",
    "    model_text = load_model('./model_checkpoint/{}-text.h5'.format(name))\n",
    "    model_image = load_model('./model_checkpoint/{}-image.h5'.format(name))\n",
    "    model_IF = load_model('./model_checkpoint/{}-IF.h5'.format(name))\n",
    "\n",
    "    y_pred_text = model_text.predict(X1_test)\n",
    "    y_pred_image = model_image.predict(X2_test)\n",
    "    y_pred_IF = model_IF.predict([X1_test, X2_test])\n",
    "    \n",
    "    y_pred = weighted_average(weights, np.asarray([y_pred_text, y_pred_image, y_pred_IF], dtype='float32'))\n",
    "    \n",
    "    best_epoch_text = np.argmin(history_text.history['val_loss'])\n",
    "    best_epoch_image = np.argmin(history_image.history['val_loss'])\n",
    "    best_epoch_IF = np.argmin(history_IF.history['val_loss'])\n",
    "\n",
    "    print('Checkpoint of text model loaded at epoch:', best_epoch_text)\n",
    "    print('Checkpoint of image model loaded at epoch:', best_epoch_image)\n",
    "    print('Checkpoint of IF model loaded at epoch:', best_epoch_IF)\n",
    "\n",
    "    return evaluate_LF(y_test, y_pred, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8411e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.139045Z",
     "iopub.status.busy": "2022-07-01T14:13:57.138707Z",
     "iopub.status.idle": "2022-07-01T14:13:57.145915Z",
     "shell.execute_reply": "2022-07-01T14:13:57.144945Z"
    },
    "papermill": {
     "duration": 0.017527,
     "end_time": "2022-07-01T14:13:57.147783",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.130256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_LF(y_true, y_pred, verbose=0):\n",
    "    \n",
    "    y_pred = le.inverse_transform(y_pred.argmax(axis=1))\n",
    "    y_true = le.inverse_transform(y_true.argmax(axis=1))\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        matrix = confusion_matrix(y_true, y_pred,\n",
    "                                  labels=list(le.classes_))\n",
    "        cm_disp = ConfusionMatrixDisplay(confusion_matrix=matrix,\n",
    "                                  display_labels=list(le.classes_))\n",
    "        cm_disp.plot()\n",
    "        plt.show()\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45431ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.164815Z",
     "iopub.status.busy": "2022-07-01T14:13:57.163869Z",
     "iopub.status.idle": "2022-07-01T14:13:57.173303Z",
     "shell.execute_reply": "2022-07-01T14:13:57.172405Z"
    },
    "papermill": {
     "duration": 0.020357,
     "end_time": "2022-07-01T14:13:57.175456",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.155099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('LOSS')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='best')\n",
    "\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('ACCURACY')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='best')\n",
    "\n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.plot(history.history['f1_score'])\n",
    "    plt.plot(history.history['val_f1_score'])\n",
    "    plt.title('F1-SCORE')\n",
    "    plt.ylabel('f1-score')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bb53658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.192530Z",
     "iopub.status.busy": "2022-07-01T14:13:57.191888Z",
     "iopub.status.idle": "2022-07-01T14:13:57.197707Z",
     "shell.execute_reply": "2022-07-01T14:13:57.196698Z"
    },
    "papermill": {
     "duration": 0.016383,
     "end_time": "2022-07-01T14:13:57.199685",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.183302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def style_dataframe(dataframe):\n",
    "    return dataframe.style.highlight_max(subset=['Accuracy', 'F1-score'], props='color:lawngreen', axis=0)\\\n",
    "                          .highlight_min(subset=['Accuracy', 'F1-score'], props='color:tomato', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "289853ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.216656Z",
     "iopub.status.busy": "2022-07-01T14:13:57.215974Z",
     "iopub.status.idle": "2022-07-01T14:13:57.223436Z",
     "shell.execute_reply": "2022-07-01T14:13:57.222394Z"
    },
    "papermill": {
     "duration": 0.018337,
     "end_time": "2022-07-01T14:13:57.225462",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.207125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "def display_dataframes(dfs, names=[], index=False):\n",
    "    def to_df(x):\n",
    "        if isinstance(x, pd.Series):\n",
    "            return pd.DataFrame(x)\n",
    "        else:\n",
    "            return x\n",
    "    html_str = ''\n",
    "    if names:\n",
    "        html_str += ('<tr>' + \n",
    "                     ''.join(f'<td style=\"text-align:center\">{name}</td>' for name in names) + \n",
    "                     '</tr>')\n",
    "    html_str += ('<tr>' + \n",
    "                 ''.join(f'<td style=\"vertical-align:top\"> {to_df(df).to_html()}</td>' \n",
    "                         for df in dfs) + \n",
    "                 '</tr>')\n",
    "    html_str = f'<table>{html_str}</table>'\n",
    "    html_str = html_str.replace('table','table style=\"display:inline\"')\n",
    "    display_html(html_str, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c0c94",
   "metadata": {
    "papermill": {
     "duration": 0.007084,
     "end_time": "2022-07-01T14:13:57.239994",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.232910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c2afaa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:13:57.257036Z",
     "iopub.status.busy": "2022-07-01T14:13:57.256229Z",
     "iopub.status.idle": "2022-07-01T14:14:11.793411Z",
     "shell.execute_reply": "2022-07-01T14:14:11.791952Z"
    },
    "papermill": {
     "duration": 14.549111,
     "end_time": "2022-07-01T14:14:11.796409",
     "exception": false,
     "start_time": "2022-07-01T14:13:57.247298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Choose best\n",
    "# Load text feature\n",
    "mvsa_single_bert, mvsa_multiple_bert = load_mvsa_feature('bert-base')\n",
    "mvsa_single_pos_bow, mvsa_multiple_pos_bow = load_mvsa_feature('pos-bow')\n",
    "mvsa_single_pos_tfidf, mvsa_multiple_pos_tfidf = load_mvsa_feature('pos-tfidf')\n",
    "mvsa_single_ner_bow, mvsa_multiple_ner_bow = load_mvsa_feature('ner-bow')\n",
    "mvsa_single_ner_tfidf, mvsa_multiple_ner_tfidf = load_mvsa_feature('ner-tfidf')\n",
    "\n",
    "## Load image feature\n",
    "mvsa_single_vgg16, mvsa_multiple_vgg16 = load_mvsa_feature('vgg16')\n",
    "mvsa_single_vgg19, mvsa_multiple_vgg19 = load_mvsa_feature('vgg19')\n",
    "mvsa_single_resnet50, mvsa_multiple_resnet50 = load_mvsa_feature('resnet50')\n",
    "mvsa_single_resnet101, mvsa_multiple_resnet101 = load_mvsa_feature('resnet101')\n",
    "mvsa_single_resnet152, mvsa_multiple_resnet152 = load_mvsa_feature('resnet152')\n",
    "mvsa_single_densenet121, mvsa_multiple_densenet121 = load_mvsa_feature('densenet121')\n",
    "mvsa_single_densenet169, mvsa_multiple_densenet169 = load_mvsa_feature('densenet169')\n",
    "mvsa_single_densenet201, mvsa_multiple_densenet201 = load_mvsa_feature('densenet201')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9af421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:11.825605Z",
     "iopub.status.busy": "2022-07-01T14:14:11.825151Z",
     "iopub.status.idle": "2022-07-01T14:14:11.924976Z",
     "shell.execute_reply": "2022-07-01T14:14:11.923793Z"
    },
    "papermill": {
     "duration": 0.117249,
     "end_time": "2022-07-01T14:14:11.927929",
     "exception": false,
     "start_time": "2022-07-01T14:14:11.810680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mvsa_single_bert_pos = np.concatenate((mvsa_single_bert, mvsa_single_pos_tfidf), axis=1)\n",
    "mvsa_single_bert_pos_ner = np.concatenate((mvsa_single_bert, mvsa_single_pos_tfidf, mvsa_single_ner_tfidf), axis=1)\n",
    "\n",
    "mvsa_multiple_bert_pos = np.concatenate((mvsa_multiple_bert, mvsa_multiple_pos_tfidf), axis=1)\n",
    "mvsa_multiple_bert_pos_ner = np.concatenate((mvsa_multiple_bert, mvsa_multiple_pos_tfidf, mvsa_multiple_ner_tfidf), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dd7dbff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:11.954881Z",
     "iopub.status.busy": "2022-07-01T14:14:11.954413Z",
     "iopub.status.idle": "2022-07-01T14:14:12.180915Z",
     "shell.execute_reply": "2022-07-01T14:14:12.179920Z"
    },
    "papermill": {
     "duration": 0.243304,
     "end_time": "2022-07-01T14:14:12.183646",
     "exception": false,
     "start_time": "2022-07-01T14:14:11.940342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mvsa_single_multimodal_labels, mvsa_single_text_labels, mvsa_single_image_labels = load_labels('../input/mvsa-features/labels/mvsa-single-labels.hdf5')\n",
    "mvsa_multiple_multimodal_labels, mvsa_multiple_text_labels, mvsa_multiple_image_labels = load_labels('../input/mvsa-features/labels/mvsa-multiple-labels.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a3b0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:12.201329Z",
     "iopub.status.busy": "2022-07-01T14:14:12.200352Z",
     "iopub.status.idle": "2022-07-01T14:14:12.248039Z",
     "shell.execute_reply": "2022-07-01T14:14:12.246896Z"
    },
    "papermill": {
     "duration": 0.059738,
     "end_time": "2022-07-01T14:14:12.251109",
     "exception": false,
     "start_time": "2022-07-01T14:14:12.191371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vector shape: (802,)\n",
      "Image vector shape: (1920,)\n"
     ]
    }
   ],
   "source": [
    "mvsa_single_text_features = np.concatenate((mvsa_single_bert, mvsa_single_pos_tfidf), axis=1)\n",
    "mvsa_multiple_text_features = np.concatenate((mvsa_multiple_bert, mvsa_multiple_pos_tfidf), axis=1)\n",
    "\n",
    "mvsa_single_image_features = mvsa_single_densenet201\n",
    "mvsa_multiple_image_features = mvsa_multiple_densenet201\n",
    "\n",
    "print('Text vector shape:', mvsa_single_text_features.shape[1:])\n",
    "print('Image vector shape:', mvsa_single_image_features.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a62123d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:12.268602Z",
     "iopub.status.busy": "2022-07-01T14:14:12.267939Z",
     "iopub.status.idle": "2022-07-01T14:14:12.274006Z",
     "shell.execute_reply": "2022-07-01T14:14:12.273003Z"
    },
    "papermill": {
     "duration": 0.016995,
     "end_time": "2022-07-01T14:14:12.275975",
     "exception": false,
     "start_time": "2022-07-01T14:14:12.258980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare all features data\n",
    "\n",
    "feature_names = ['resnet152-bert', 'resnet152-bert-lstm', 'densenet201-bert-pos-lstm', 'densenet201-bert-pos-ner-lstm']\n",
    "\n",
    "mvsa_single_features = [[mvsa_single_bert, mvsa_single_resnet152],\n",
    "                        [mvsa_single_bert, mvsa_single_resnet152],\n",
    "                        [mvsa_single_bert_pos, mvsa_single_densenet201],\n",
    "                        [mvsa_single_bert_pos_ner, mvsa_single_densenet201]]\n",
    "\n",
    "mvsa_multiple_features = [[mvsa_multiple_bert, mvsa_multiple_resnet152],\n",
    "                          [mvsa_multiple_bert, mvsa_multiple_resnet152],\n",
    "                          [mvsa_multiple_bert_pos, mvsa_multiple_densenet201],\n",
    "                          [mvsa_multiple_bert_pos_ner, mvsa_multiple_densenet201]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc9dbc8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:12.293727Z",
     "iopub.status.busy": "2022-07-01T14:14:12.292663Z",
     "iopub.status.idle": "2022-07-01T14:14:12.299256Z",
     "shell.execute_reply": "2022-07-01T14:14:12.298364Z"
    },
    "papermill": {
     "duration": 0.017769,
     "end_time": "2022-07-01T14:14:12.301458",
     "exception": false,
     "start_time": "2022-07-01T14:14:12.283689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(mvsa_single_multimodal_labels)\n",
    "NUM_CLASSES = len(le.classes_) # = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a1c3b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:12.318008Z",
     "iopub.status.busy": "2022-07-01T14:14:12.317680Z",
     "iopub.status.idle": "2022-07-01T14:14:12.322660Z",
     "shell.execute_reply": "2022-07-01T14:14:12.321554Z"
    },
    "papermill": {
     "duration": 0.01623,
     "end_time": "2022-07-01T14:14:12.325244",
     "exception": false,
     "start_time": "2022-07-01T14:14:12.309014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reset_seeds()\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EARLY_STOPPING = 100\n",
    "NUM_LSTM = 128\n",
    "DROPOUT_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f694d49d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:14:12.342767Z",
     "iopub.status.busy": "2022-07-01T14:14:12.342357Z",
     "iopub.status.idle": "2022-07-01T14:17:43.976953Z",
     "shell.execute_reply": "2022-07-01T14:17:43.975507Z"
    },
    "papermill": {
     "duration": 211.655104,
     "end_time": "2022-07-01T14:17:43.988305",
     "exception": false,
     "start_time": "2022-07-01T14:14:12.333201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVSA-Single\n",
      "\n",
      "MVSA-Single: resnet152-bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 14:14:12.440562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:12.441719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:12.442405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:12.443315: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-01 14:14:12.443652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:12.444347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:12.445024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:17.271092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:17.271995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:17.272654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 14:14:17.273228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15047 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-07-01 14:14:17.945759: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint of text model loaded at epoch: 37\n",
      "Checkpoint of image model loaded at epoch: 93\n",
      "Checkpoint of IF model loaded at epoch: 57\n",
      "\n",
      "MVSA-Single: resnet152-bert-lstm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 14:15:03.816508: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint of text model loaded at epoch: 10\n",
      "Checkpoint of image model loaded at epoch: 93\n",
      "Checkpoint of IF model loaded at epoch: 62\n",
      "\n",
      "MVSA-Single: densenet201-bert-pos-lstm\n",
      "Checkpoint of text model loaded at epoch: 6\n",
      "Checkpoint of image model loaded at epoch: 6\n",
      "Checkpoint of IF model loaded at epoch: 5\n",
      "\n",
      "MVSA-Single: densenet201-bert-pos-ner-lstm\n",
      "Checkpoint of text model loaded at epoch: 6\n",
      "Checkpoint of image model loaded at epoch: 5\n",
      "Checkpoint of IF model loaded at epoch: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MVSA-Single\\n')\n",
    "# run model with all features data of MVSA-Single\n",
    "mvsa_single_scores = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    print('MVSA-Single:', feature_names[i])\n",
    "    if 'lstm' in feature_names[i]:\n",
    "        scores = run_and_evaluate_HF('single-' + feature_names[i], mvsa_single_features[i][0], mvsa_single_features[i][1], \n",
    "                                     mvsa_single_multimodal_labels, mvsa_single_text_labels, mvsa_single_image_labels,\n",
    "                                     verbose=0)\n",
    "    else:\n",
    "        scores = run_and_evaluate_HF_no_lstm('single-' + feature_names[i], mvsa_single_features[i][0], mvsa_single_features[i][1],\n",
    "                                             mvsa_single_multimodal_labels, mvsa_single_text_labels, mvsa_single_image_labels,\n",
    "                                             verbose=0)\n",
    "    mvsa_single_scores.append(scores)\n",
    "    print()\n",
    "\n",
    "df_single_scores = pd.DataFrame(mvsa_single_scores, columns=['Accuracy', 'F1-score'], index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c762c8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:17:44.007704Z",
     "iopub.status.busy": "2022-07-01T14:17:44.006590Z",
     "iopub.status.idle": "2022-07-01T14:26:43.217271Z",
     "shell.execute_reply": "2022-07-01T14:26:43.212564Z"
    },
    "papermill": {
     "duration": 539.228603,
     "end_time": "2022-07-01T14:26:43.225445",
     "exception": false,
     "start_time": "2022-07-01T14:17:43.996842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVSA-Multiple\n",
      "\n",
      "MVSA-Multiple: resnet152-bert\n",
      "Checkpoint of text model loaded at epoch: 13\n",
      "Checkpoint of image model loaded at epoch: 24\n",
      "Checkpoint of IF model loaded at epoch: 10\n",
      "\n",
      "MVSA-Multiple: resnet152-bert-lstm\n",
      "Checkpoint of text model loaded at epoch: 2\n",
      "Checkpoint of image model loaded at epoch: 24\n",
      "Checkpoint of IF model loaded at epoch: 76\n",
      "\n",
      "MVSA-Multiple: densenet201-bert-pos-lstm\n",
      "Checkpoint of text model loaded at epoch: 2\n",
      "Checkpoint of image model loaded at epoch: 2\n",
      "Checkpoint of IF model loaded at epoch: 2\n",
      "\n",
      "MVSA-Multiple: densenet201-bert-pos-ner-lstm\n",
      "Checkpoint of text model loaded at epoch: 3\n",
      "Checkpoint of image model loaded at epoch: 2\n",
      "Checkpoint of IF model loaded at epoch: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MVSA-Multiple\\n')\n",
    "# run model with all features data of MVSA-Single\n",
    "mvsa_multiple_scores = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    print('MVSA-Multiple:', feature_names[i])\n",
    "    if 'lstm' in feature_names[i]:\n",
    "        scores = run_and_evaluate_HF('multiple-' + feature_names[i], mvsa_multiple_features[i][0], mvsa_multiple_features[i][1], \n",
    "                                     mvsa_multiple_multimodal_labels, mvsa_multiple_text_labels, mvsa_multiple_image_labels,\n",
    "                                     verbose=0)\n",
    "    else:\n",
    "        scores = run_and_evaluate_HF_no_lstm('multiple-' + feature_names[i], mvsa_multiple_features[i][0], mvsa_multiple_features[i][1],\n",
    "                                             mvsa_multiple_multimodal_labels, mvsa_multiple_text_labels, mvsa_multiple_image_labels,\n",
    "                                             verbose=0)\n",
    "    mvsa_multiple_scores.append(scores)\n",
    "    print()\n",
    "\n",
    "df_multiple_scores = pd.DataFrame(mvsa_multiple_scores, columns=['Accuracy', 'F1-score'], index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96b44650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-01T14:26:43.243825Z",
     "iopub.status.busy": "2022-07-01T14:26:43.243462Z",
     "iopub.status.idle": "2022-07-01T14:26:43.327537Z",
     "shell.execute_reply": "2022-07-01T14:26:43.326399Z"
    },
    "papermill": {
     "duration": 0.096365,
     "end_time": "2022-07-01T14:26:43.330447",
     "exception": false,
     "start_time": "2022-07-01T14:26:43.234082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\"><tr><td style=\"text-align:center\">MVSA-Single</td><td style=\"text-align:center\">MVSA-Multiple</td></tr><tr><td style=\"vertical-align:top\"> <style type=\"text/css\">\n",
       "#T_97d13_row1_col0, #T_97d13_row1_col1 {\n",
       "  color: tomato;\n",
       "}\n",
       "#T_97d13_row2_col0, #T_97d13_row3_col1 {\n",
       "  color: lawngreen;\n",
       "}\n",
       "</style>\n",
       "<table style=\"display:inline\" id=\"T_97d13_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th class=\"col_heading level0 col1\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_97d13_level0_row0\" class=\"row_heading level0 row0\" >resnet152-bert</th>\n",
       "      <td id=\"T_97d13_row0_col0\" class=\"data row0 col0\" >0.577726</td>\n",
       "      <td id=\"T_97d13_row0_col1\" class=\"data row0 col1\" >0.443661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97d13_level0_row1\" class=\"row_heading level0 row1\" >resnet152-bert-lstm</th>\n",
       "      <td id=\"T_97d13_row1_col0\" class=\"data row1 col0\" >0.535963</td>\n",
       "      <td id=\"T_97d13_row1_col1\" class=\"data row1 col1\" >0.393433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97d13_level0_row2\" class=\"row_heading level0 row2\" >densenet201-bert-pos-lstm</th>\n",
       "      <td id=\"T_97d13_row2_col0\" class=\"data row2 col0\" >0.642691</td>\n",
       "      <td id=\"T_97d13_row2_col1\" class=\"data row2 col1\" >0.487239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97d13_level0_row3\" class=\"row_heading level0 row3\" >densenet201-bert-pos-ner-lstm</th>\n",
       "      <td id=\"T_97d13_row3_col0\" class=\"data row3 col0\" >0.638051</td>\n",
       "      <td id=\"T_97d13_row3_col1\" class=\"data row3 col1\" >0.499243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">\n",
       "</td><td style=\"vertical-align:top\"> <style type=\"text/css\">\n",
       "#T_07852_row0_col0, #T_07852_row0_col1 {\n",
       "  color: tomato;\n",
       "}\n",
       "#T_07852_row2_col0, #T_07852_row2_col1 {\n",
       "  color: lawngreen;\n",
       "}\n",
       "</style>\n",
       "<table style=\"display:inline\" id=\"T_07852_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th class=\"col_heading level0 col1\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_07852_level0_row0\" class=\"row_heading level0 row0\" >resnet152-bert</th>\n",
       "      <td id=\"T_07852_row0_col0\" class=\"data row0 col0\" >0.639194</td>\n",
       "      <td id=\"T_07852_row0_col1\" class=\"data row0 col1\" >0.286297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_07852_level0_row1\" class=\"row_heading level0 row1\" >resnet152-bert-lstm</th>\n",
       "      <td id=\"T_07852_row1_col0\" class=\"data row1 col0\" >0.641636</td>\n",
       "      <td id=\"T_07852_row1_col1\" class=\"data row1 col1\" >0.294535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_07852_level0_row2\" class=\"row_heading level0 row2\" >densenet201-bert-pos-lstm</th>\n",
       "      <td id=\"T_07852_row2_col0\" class=\"data row2 col0\" >0.671551</td>\n",
       "      <td id=\"T_07852_row2_col1\" class=\"data row2 col1\" >0.484460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_07852_level0_row3\" class=\"row_heading level0 row3\" >densenet201-bert-pos-ner-lstm</th>\n",
       "      <td id=\"T_07852_row3_col0\" class=\"data row3 col0\" >0.659951</td>\n",
       "      <td id=\"T_07852_row3_col1\" class=\"data row3 col1\" >0.481986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">\n",
       "</td></tr></table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_dataframes((style_dataframe(df_single_scores), style_dataframe(df_multiple_scores)), \n",
    "                   names=['MVSA-Single', 'MVSA-Multiple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe3e7c",
   "metadata": {
    "papermill": {
     "duration": 0.008112,
     "end_time": "2022-07-01T14:26:43.347305",
     "exception": false,
     "start_time": "2022-07-01T14:26:43.339193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 787.744555,
   "end_time": "2022-07-01T14:26:46.840989",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-01T14:13:39.096434",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
